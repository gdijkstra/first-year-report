\documentclass[a4paper,10pt]{report}

%include report.fmt

\newcommand{\todoi}[1]{\todo[inline]{#1}}

\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10,protrusion=trues]{microtype}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[parfill]{parskip}
\usepackage{amsmath,amsthm,amssymb,stmaryrd}
\usepackage{todonotes}
\usepackage{tgpagella}
\usepackage{url}
\usepackage{xspace}

% Bibliography stuff
\usepackage[autostyle]{csquotes}

\usepackage[
    backend=biber,
    style=authoryear-icomp,
    natbib=true,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\addbibresource{latex-base/refs.bib}

\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing} % for snake lines
\usetikzlibrary{matrix}
% TikZ styles for drawing

\tikzstyle{snakeline} = [->, decorate, decoration={zigzag, pre length=0.1cm, post length=0.1cm, segment length=1.5mm, amplitude=.25mm}]
\tikzstyle{snakelin} = [->, decoration={zigzag, pre length=0.1cm, post length=0.1cm, segment length=1.5mm, amplitude=.25mm}]
\tikzset{node distance=2cm, auto}

\newcommand{\withoutk}{\xspace\verb+--without-K+\xspace}
\newcommand{\hott}{homotopy type theory\xspace}
\newcommand{\Hott}{Homotopy type theory\xspace}
\newcommand{\mltt}{Martin-L\"of's type theory\xspace}
\newcommand{\hit}{higher inductive type\xspace}
\newcommand{\Hit}{Higher inductive type\xspace}
\newcommand{\hits}{higher inductive types\xspace}
\newcommand{\Hits}{Higher inductive types\xspace}
\newcommand{\onehit}{$1$-HIT\xspace}
\newcommand{\onehits}{$1$-HITs\xspace}
\newcommand{\oneconstructor}{$1$-constructor\xspace}
\newcommand{\oneconstructors}{$1$-constructors\xspace}
\newcommand{\twohit}{$2$-HIT\xspace}
\newcommand{\twohits}{$2$-HITs\xspace}
\newcommand{\twoconstructor}{$2$-constructor\xspace}
\newcommand{\twoconstructors}{$2$-constructors\xspace}
\newcommand{\ie}{i.e.\xspace}

\title{Pattern matching in homotopy type theory}

\author{First year report \\\\ Gabe Dijkstra}

\date{\today}

\begin{document}

\maketitle

%if False
\begin{code}
module report where
\end{code}
%endif

\tableofcontents

\chapter*{Preface}
\label{ch:preface}
\addcontentsline{toc}{chapter}{Preface}

dingen foo

\chapter{Homotopy type theory}

Short introduction to homotopy type theory.

\chapter{Higher inductive types}
\label{ch:hit}

\Hott teaches us that we can regard types as topological spaces up to
homotopy. This leads to the question of how we can represent familiar
spaces such as the circle and the torus. So far several examples of
what \hits may look like have been explored, \todo{Citations} but
formal scheme has been devised as of yet. In this chapter we will
explore ways to formalise (restricted) notions of higher inductive
types.

\todoi{More motivation}

\section{Examples}

The idea of \hits is that apart from providing ways of constructing
points, we also provide ways of constructing paths. For example, the
circle can be defined as the type generated by one point and one
non-trivial loop on that point: we define |Circle| to be the type with:

\begin{itemize}
\item \emph{point} constructor |base : Circle|
\item \emph{path} constructor |loop : base == base|
\end{itemize}

In order to eliminate out of this type, we need to provide a point in
our codomain and ensure that this point also has a corresponding loop:

\begin{code}
  Circle-elim : (Z : Type0) (zb : Z) (zloop : zb == zb) -> Circle -> Z
\end{code}

As with ordinary inductive types, the elimination principle should
also satisfy certain computation rules:

\begin{code}
  Circle-elim-comp-point  : (Z : Type0) (zb : Z) (zloop : zb == zb)
                          -> Circle-elim Z zb zloop base == zb
\end{code}

Apart from computation rules on points, we also need computation rules
on paths, witnessing the fact that every function defined in type
theory respects equality:

\begin{code}
  Circle-elim-comp-path  : (Z : Type0) (zb : Z) (zloop : zb == zb)
                         -> ap (Circle-elim Z zb zloop) == zloop
\end{code}

Apart from having path constructors between point constructors, one
can also have \emph{variables} of the type being defined in left or
right hand sides of the equations. An example of this is propositional
truncation, or $(-1)$-truncation. Given |A : Type0|, the propositional
truncation of |A|, called |// A //| is defined by:

\begin{itemize}
\item a point constructor |[ _ ] : A -> // A //|
\item a path constructor |trunc : (x y : // A //) -> x == y|
\end{itemize}

with the elimination principle:

\begin{code}
  // A // - elim : (Z : Type0) (f : A -> Z) (trunc : (x y : Z) -> x == y) -> // A // -> Z
\end{code}

The two examples we have seen so far are special cases of \hits,
called \onehits, as the path constructors describe paths between
points of the type being defined, hence can be called
\emph{\oneconstructors} (with ordinary point constructors being
\emph{$0$-constructors}). We can also consider adding constructors for
paths between paths. One example of this is the following construction
of the torus, which has:

\begin{itemize}
\item a point constructor |base : Torus|
\item two non-trivial loops |p q : base == base|
\item such that their composition commutes: |t : p trans q == q trans p|
\end{itemize}

In this example one sees that the terms that can occur in the
constituents of the equations of higher path constructors are no
longer terms generated by the point constructors that can have
variables in them. They are terms built out of the path constructors
or variables from one level down, which can be combined with the
groupoid operations that come with equalities: transitivity, symmetry
and reflexivity.

One example in which the reflexivity term occurs in a $2$-constructor,
is $0$-truncation, or set truncation, which is given by:

\begin{itemize}
\item a point constructor |[ _ truncend0 : A -> // A ///|
\item a path constructor |trunc0 : (x : // A ///) (p : x == x) -> p == refl|
\end{itemize}

The path constructor forces axiom K to hold for the type |// A ///|,
making it a set.

\subsection{Hub-spokes construction}

It has been observed that one actually can do without the higher path
constructors: every $n$-HIT can be reduced to a \onehit. For example,
the disc may be realised as a circle with an added point constructors
(the hub) along with a continuous function of paths from points on the
circle to the hub (the spokes). 

\section{Containers}

Before we can start formalising a notion of \hits, we need to know
what ordinary inductive types are. One way to look at inductive types
is as initial algebras of strictly positive functors. For example, the
list data type can be seen as the initial algebra of the functor
(having fixed some |A : Type0|) |ListFA X = 1 + A times X|. An algebra
for this functor is a type |Z : Type0| along with a function |ListF Z
-> Z|. An initial algebra |List : Type0| with |sup : ListF List ->
List| then satisfies the following universal property for any algebra
|theta : ListF Z -> Z| with |Z : Type0|:

\begin{center}
  \begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
 { |ListF List| & & |List| \\
   |ListF Z|    & & |Z|    \\ };

  \path[->] (m-1-1) edge[decorate] node[auto]       {|sup|} (m-1-3);
  \path[->] (m-1-1) edge[decorate] node[auto,swap]  {|F (elim theta)|} (m-2-1);
  \path[->] (m-1-3) edge[decorate] node[auto]       {|elim theta|} (m-2-3);
  \path[->] (m-2-1) edge[decorate] node[auto]       {|theta|} (m-2-3);
  \end{tikzpicture}
\end{center}

Unfolding the definition of |ListF| in the type of |sup| and applying
the universal property of sums shows us that |sup| is exactly the
familiar list constructors |nil| and |cons| combined. The existence of
|elim theta| for any |theta| gives us the non-dependent elimination
principle, which in this case, if we unfold the definition of |ListF|,
gives, we see that this is the familiar |fold| operation on lists. The
commutativity of the diagram gives us the computation rules for
|elim|.

Initiality also means that two algebras satisfying the universal
property are isomorphic, so we can fix a candidate and talk about
\emph{the} initial algebra of a strictly positive functor.

The condition on functors that they need to be strictly positive is
necessary: if we allow for recursion to happen in negative positions,
we can write terms that do not normalise. We can encode the
non-normalising |(\ x . x x) (\ x . x x)| combinator from the untyped
$\lambda$-calculus using the initial algebra of the functor |F X = X
-> X|. Let us call the initial algebra of |F| |Term|, with its
constructor being |Lam : Term -> Term|.

\begin{code}
  omega : Term -> Term
  omega (Lam x) := x (Lam x)

  Omega : Term
  Omega = omega (Lam omega)
\end{code}

The condition that functors may not have recursive occurrences in
negative positions can be described as that the functor can be written
as using the following grammar:

\todoi{grammar from that one paper}

Formalising this grammar has several problems. Dealing with binders is
usually annoying as one has to use things such as de Bruijn
indices. One also has the ``problem'' that there are multiple
representations of the same functor: the constant functor can be
described as |`1| or |`1 `+ `0|, etc.

Another way to more succinctly formalise the notion of strictly
positive functors is as \emph{containers} \citep{Abbott2005}: a
container is a pair |S : Type0|, the type of \emph{shapes}, and |P : S
-> Type0|, the type of \emph{positions}, denoted |S lhd P|. The
container then describes the functor |sem(S lhd P) X = (s : S) times
(P s -> X)|, which is sometimes called the \emph{extension} of the
container |S lhd P|. The action on morphisms of the extension can then
be defined as follows:

\begin{code}
  map : {C : Cont} {A B : Type0} (f : A -> B) -> sem(C) A -> sem(C) B
  map f (s , u) := (s , \ i -> f (u i))
\end{code}

The pros \todo{need better word for this} of this approach are that it
is easier to formalise in languages such as Agda and that if we have
two containers such that the extensions are equal, then the components
must be equal as well.

\subsection{Initial algebra of a container}

Containers give us a nice description of strictly positive functors,
but we are mainly interested in their initial algebras. Suppose |sup :
sem(C) WC -> WC| is an initial algebra of the functor |sem(C)| for
some container |C|, then we have that the following diagram commutes
for any other algebra |theta : sem(C) Z -> Z|:

\begin{center}
  \begin{tikzpicture}
    \node (CWC)               {|sem(C) WC|};
    \node (CZ) [below of=CWC] {|sem(C) Z|};
    \node (WC) [right of=CWC] {|WC|};
    \node (Z)  [below of= WC] {|Z|};
    
    \draw[->] (CWC) to node {|sup|} (WC);
    \draw[->] (CWC) to node[swap] {|map C (elim theta)|} (CZ);
    \draw[->] (CZ) to node {|theta|} (Z);
    \draw[->] (WC) to node {|elim theta|} (Z);
  \end{tikzpicture}
\end{center}


If we unfold the definition of |sem(C)| we get the following:

\begin{itemize}
\item Term introduction: |sup : (s : S) times (P s -> WC) -> WC|

\item Elimination principle: |elim : {Z : Type0} (theta : s : S times
  (P s -> Z) -> WC -> Z|

\item Computation rule: for every |s : S|, |u : P s -> WC| and algebra
  |theta : (s : S) times (P s -> Z) -> Z| we get |elim theta (sup (s,
  u)) = theta (s, \ i -> elim theta (u i))|
\end{itemize}

which is the familiar \emph{W-type} construction from \todo{martin-lof
  citation}

Using this we can define the natural numbers as the initial algebra of
the following container:

\begin{code}
  S : Type0
  S := 1 + 1

  P : Type0
  P (inl tt) := 0
  P (inr tt) := 1
\end{code}

The |zero| and |succ| constructors can be defined as follows:

\begin{code}
  zero : WC
  zero := sup (inl tt) exfalso

  succ : WC -> WC
  succ n := sup (inr tt) (\ x -> n)
\end{code}

\todoi{Note something about the lack of unicity of the
  constructors. One needs to compare functions from an empty type into
  some other type. Adding this functionality to the type theory
  quickly makes the system undecidable.}

\subsection{Natural transformations}

We can also easily characterise natural transformations between
containers. Suppose |C| and |D| are containers, then a natural
transformation |eta : sem(C) -> sem(D)| is a collection of morphisms
|eta : (X : Type0) -> sem(C) X -> sem(D) X|, such that for any |f : X -> Y|
the following commutes:

\begin{center}
  \begin{tikzpicture}
    \node (CX)               {|sem(C) X|};
    \node (CY) [below of=CX] {|sem(C) Y|};
    \node (DX) [right of=CX] {|sem(D) X|};
    \node (DY) [below of=DX] {|sem(D) Y|};
    
    \draw[->] (CX) to node[swap] {|map C f|} (CY);
    \draw[->] (DX) to node {|map D f|} (DY);
    \draw[->] (CX) to node {|eta X|} (DX);
    \draw[->] (CY) to node {|eta Y|} (DY);
  \end{tikzpicture}
\end{center}

If one unfolds the definitions and follows the types, one notices that
a natural transformation can be defined by providing a mapping between
the types of shapes, |f : S0 -> S1|, and a mapping between the
positions in the other direction: |g : (s : S0) -> P1 (f s) -> P0
s|. Applying the natural transformation can then be done as follows:

\begin{code}
  apply-nat  :   {X : Type0} {C D : Cont} 
                 (f : S0 -> S1)
                 (g : (s : S0) -> P1 (f s) -> P0 s) 
             ->  sem(C) X -> sem(D) X
  apply-nat f g (s , u) := (f s , \i -> u (g (f s) i))
\end{code}

The naturality of |apply-nat| can be easily checked for any |f| and
|g| by $\beta$-reduction.

\subsection{Free monads}

If we want to describe the left and right hand sides of the equations
in a \oneconstructor, we need some way to talk about terms of a data
type with possibly free variables. One representation of this is given
by the free monad of the functor of which the data type at hand is the
initial algebra.

Given a container, we can construct the free monad of the extension of
the container as a new data type |Free|, given parameters |C : Cont|
and |X : Type0|, with point constructors:

\begin{itemize}
\item |eta : X -> Free C X|
\item |roll : sem(C) (Free C X) -> Free C X|
\end{itemize}

The |join| operation can be defined as follows:

\begin{code}
  join : {C : Cont} {X : Type0} -> Free C (Free C X) -> Free C X
  join {C} (eta c)   := c
  join {C} (roll x)  := roll (map C join x)
\end{code}

Note that if we inline the definition of |map|, we notice that this
definition above is indeed structurally recursive.

This data type can of course be expressed as a W-type, but we will use
this representation for readability. Both approaches give us an
operation of type |Cont -> Type0 -> Type0| instead of |Cont -> Cont|,
which means we cannot reuse the definition of natural transformation
and the action on morphisms for this. However, we can use the |Free|
data type to create a new container that describes the free monad. We
know that |sem(S lhd P) top = (s : S) times P s -> top == S|. Using
this fact, given a container |S lhd P|, we can produce a new
container, where |P*| is defined by structural recursion:

\begin{code}
    S' : Type0
    S' := Free C top

    P' : S' -> Type0
    P' (ret tt)        := top
    P' (roll (s , u))  := (i : P s) times P' (u i)
\end{code}

We can now define an operation |_* : Cont -> Cont|.

\subsubsection{Algebras for a free monad}

The free monads we are working with happen to be
\emph{algebraically-free}: there is an equivalence between the
category of algebras on |sem(C)| and the category of monad algebras on
|sem(C *)|. This fact is witnessed by the two operations:

\begin{code}
  lift : {C : Cont} {Z : Type0} -> (theta : sem(C) Z -> Z) -> sem(C *) Z -> Z
\end{code}

and

\begin{code}
  forget : {C : Cont} {Z : Type0} -> (theta : sem(C *) Z -> Z) -> sem(C) Z -> Z
\end{code}

\todoi{Operational understanding of these operations.}

Note that we are being a bit imprecise in the type of |forget|, the
algebra |theta| passed to |forget| needs to be a \emph{monad} algebra, \ie the
following must commute:

\begin{center}
  \begin{tikzpicture}
    \node (T2)                {|sem(C *) (sem(C *) X)|};
    \node (T)  [right of=T2] {|sem(C *) X|};
    \node (T') [below of=T2] {|sem(C *) X|};
    \node (X)   [below of=T] {|X|};

    \draw[->] (T2) to node {|map sem(C *) theta |} (T2);
    \draw[->] (T2) to node[swap] {|join|} (T');
    \draw[->] (T') to node[swap] {|theta|} (X);
    \draw[->] (T) to node {|theta|} (X);
  \end{tikzpicture}
\end{center}

\begin{center}
  \begin{tikzpicture}
    \node (X) {|X|};
    \node (T)  [right of=X] {|sem (C *) X|};
    \node (X') [below of=T] {|X|};

    \draw[->] (X) to node {|eta|} (T);
    \draw[->] (T) to node {|theta|} (X');
    \draw[->] (X) to node[swap] {|id|} (X');
  \end{tikzpicture}
\end{center}

One can easily show that |forget (lift theta) x == theta x| for any
|theta : sem (C) X -> X| and |x : sem( C *) X| by case distinction and
$\beta$-reduction. Showing that |lift (forget theta) x == theta x| for
|theta : sem (C *) X -> X| and |x : sem(C) X| is a bit more involved
and needs to use the fact that |theta| is a monad algebra.

As we have previously mentioned, the free monad of a functor can be
seen as the type of terms of the initial algebra of the same functor,
but with free variables of the given type. We have an instantiation
operation |inst| that takes a substitution |v : X -> W C| and a term
with free variables |x : sem(C *) X| and produces a |W C| by the
following composition:

\begin{center}
  \begin{tikzpicture}
    \node (CSX) {|sem(C *) X|};
    \node (CWC) [right of=CSX] {|sem(C *) (W C)|};
    \node (WC)  [right of=CWC] {|W C|};

    \draw[->] (CSX) to node {|map (C *) v|} (CWC);
    \draw[->] (CWC) to node {|lift sup|} (WC);
  \end{tikzpicture}
\end{center}

We also have a ``generalisation'' operation that goes in the other
direction:

\begin{code}
    gen : W C -> Free C X
    gen (sup x) := roll (map C gen x)
\end{code}

which satisfies the property that for every value |x : W C| and
instantation |v : X -> W C|, we have |inst v (gen x) == x|.

\section{Construction of \onehits}

We want to extend the concept of container to also be able to describe
the data needed for the \emph{path} constructors:

\begin{itemize}
\item |C0 : Cont| for the point constructors
\item |S1 : Type0| the type of shapes of the path constructors with
  |P1 : S1 -> Type0| giving the type of \emph{variables} for every
  shape.
\item |l r : (s : S1) -> sem(C0 *) (P1 s)| describe the left and right
  hand side of the path constructors, where |sem(C0 *) (P1 s)| should
  be seen as terms constructed using |W C0| with free variables coming
  from |P1 s|.
\end{itemize}

The point and path constructors are then as follows, if we call the
type we are defining |H|:

\begin{itemize}
\item |c0 : sem(C0) H -> H|
\item |c1 : (s : S1) -> (P1 s -> H) -> inst v (l s) == inst v (r s)|
\end{itemize}

The argument of type |P1 s -> H| of the path constructor can be read
as the assignment of the free variables.

To eliminate out of a \onehit, we need to say what one needs to be
done with the point constructors and show that this is done in a way
that respects the $1$-constructors. This leads us to the following
elimination principle:

\begin{code}
  elim  :  (Z : Type0)
           (theta : sem(C0) Z -> Z)
           (rho :  (s : S1) -> (v : P1 s -> Z) -> 
                   lift theta (map (C0 *) v (l s)) == lift theta (map (C0 *) v (r s)))
        -> H -> Z
\end{code}

This principle should satisfy the following computation rule on point
constructors definitionally, given |m : sem(C0) H|:

\begin{code}
  elim Z theta rho (c0 m) = theta (map C (elim theta rho) m)
\end{code}

One would expect the computation rule for paths to look something like
the following, given |s : S1| and |v : P1 s -> H|:

\begin{code}
  ap (elim Z theta rho) (c1 s v) == rho s v
\end{code}

\todoi{But the above doesn't type check.}

\subsection{Examples}

\subsubsection{Circle}

The circle has one point constructor (|base|) and one path constructor
(|loop : base == base|) that doesn't have any arguments, so |C0 := 1
lhd (\ s -> 0)| and |S1 := 1| with |P1 s := 0|. We can define |base :=
c0 (tt , \ ())| and can then describe the |loop| constructor with |l
:= r := gen (c0 (tt , \ ()))|. If we try to define |loop := c1 tt (\
())| we do not get the type we want, which is |base == base|. Instead
we get: \todoi{iets}.

\subsubsection{Propositional truncation}

Propositional truncation of a type |A : Type0| is given by one point
constructor |A -> // A //| and one path constructor |(x y : // A //)
-> x == y|. This means that |C0 := A lhd (\ s -> 0)| and |S1 := 1|
with |P1 x := Bool|. The truncation equation is given by |l x := sup
(inl true) (\ ())| and |r x := sup (inl false)|. From this data we can
define |[ x ] := c0 x (\ ())| and |trunc x y := c1 tt (\ b -> if b
then x else y) (\ ())|.

\todoi{Show that the elimination principles just work out nicely.}

\subsection{Implementation details}

\subsection{Alternative construction}

\subsection{Closure}

We cannot define things like:

\begin{code}
  data A : Type0 where
    c : // A // -> A
\end{code}

\todoi{Something about why}

\todoi{Compare with strictly-positive functors: those just
  compose/nest/whatever.}

\todoi{Something about equations appearing in arguments of path
  constructors or whatever.}

\chapter{Dependent pattern matching}
\label{ch:patmatch}

In functional programming languages such as Haskell, one defines
functions on inductive types by using pattern matching. For example,
we can define the Ackermann function as follows:

\begin{code}
  Ack 0      n      = S n
  Ack (S m)  0      = Ack m (S 0)
  Ack (S m)  (S n)  = Ack m (Ack (S m) n)
\end{code}

When defining functions on inductive types in \mltt, we only have the
type its \emph{elimination principle} at our disposal. The
(non-dependent) elimination principle for the natural numbers is the
following:

\begin{code}
  Natelim : (Z : Type0) -> Z -> (Z -> Z) -> Nat -> Z
\end{code}

with computation rules

\begin{code}
  Natelim mz ms 0      = mz
  Natelim mz ms (S n)  = ms (Natelim mz ms n)
\end{code}

The definition of |Ack| using |Natelim| looks as follows:

\begin{code}
  Ack : Nat -> Nat -> Nat
  Ack = 
    Natelim  (Nat -> Nat) 
             (\ n -> S n)
             (\ ackm -> Natelim  Nat 
                                 (ackm (S Z)) 
                                 (\ ackSmn -> ackm ackSmn))
\end{code}

which is less readable then our original definition. It would be nice
if we added pattern matching to \mltt. However, we do need to be
careful when we do this. We have to be sure that our pattern matching
definitions are total. As it turns out, it is sufficient
\citep{Coquand1992} to check that the patterns are \emph{covering} and
that the recursive calls are done on \emph{structurally smaller}
arguments.

\todoi{Elaborate on this?}

\section{Inductive families}

In dependently-typed languages we can \emph{families} of inductive
types indexed by some other type, so-called \emph{inductive
  families}. We may want to define a family of list types, indexed by
the length:

\begin{code}
  data Vec (A : Type0) : Nat -> Type0 where
    nil   : Vec A 0
    cons  : A -> (n : Nat) -> Vec A n -> Vec A (S n)
\end{code}

|Vec| is a family of types \emph{parametrised} by |A : Type0| and
\emph{indexed} by |Nat|. Parameters are distinguished from indices in
that they occur uniformly in the result type of every constructor. In
the |Vec| example, the |A| occurs uniformly in both the result type of
|nil| as well as |cons|, whereas the index of type |Nat| is |0| in the
|nil| case and |S n| in the |cons| case.

Indices are a useful way of encoding invariants in our data types. We
can get a lot more information about a value we get just from its
type. If we for example get a value |l : Vec A (S n)| for some |A :
Type0| and |n : Nat|, we know that |l| cannot be |nil|, as that would
not type check. This kind of information can also help us when we
write functions on inductive families. Say we want to define the
function |head| on vectors, a definition that only makes sense on
non-empty vectors. We can specify this constraint in the type as
follows: |head : (A : Type0) (n : Nat) -> Vec A (S n) -> A|. As
mentioned before, we should only have to define what to do in the
|cons| case, as the |nil| case is impossible. The form of pattern
matching that takes into account the extra information one gets from
the indices as they occur in the type signature of the function, is
called \emph{dependent pattern matching}.

With dependent pattern matching, we can write |head| as follows:

\begin{code}
  head : (A : Type0) (n : Nat) -> Vec A (S n) -> A
  head A .(S n) (cons x n xs) = x
\end{code}

The dot in the pattern |.(S n)| indicates that there is no other
possible well-typed pattern other than |S n| for that argument. The
definition is also complete: the |nil| case is impossible as we have
previously mentioned.

\section{Uniqueness of identity proofs}

One important example of an indexed family is the identity type:

\begin{code}
data Id (A : Type0) (x : A) : A -> Type0 where
  refl : Id A x x
\end{code}

An inhabitant of |Id A x y| constitutes a proof that the terms |x| and
|y| are equal. Note that |refl : Id A x y| only type checks if |x| and
|y| are definitionally equal. To increase readability, we will
sometimes denote |Id A x y| by |x == y|, leaving the type argument
implicit.

Given this definition of equality, it might lead one to think that if
we have a proof |p : Id A x y|, it is necessarily unique. In fact,
using dependent pattern matching, we can easily prove this:

\begin{code}
  uip : (A : Type0) (x y : A) (p : x == y) -> p == refl
  uip A x .x refl = refl
\end{code}

Proving this using only the elimination principle of identity types
turns out to be impossible: one can construct models of \mltt with
identity types in which the uniquenes of identity proofs property does
not hold for every type. In \hott people are specifically interested
in the models that violate this principle, hence dependent pattern
matching in its unrestricted form is incompatible with \hott. We hope
to uncover what the root of this incompatibility is and whether we can
find a variation of dependent pattern matching that is not at odds
with \hott.

\section{Parametrising inductive families}

To get a better understanding of indexed familes and dependent pattern
matching, we will translate some definitions to a non-indexed,
parametrised family, using identity types. With identity types, one
can factor out the ``indexedness'' of a family and make it more
explicit what is going on when we try to eliminate out of such a
family.

For example, we can translate the vector type as follows:

\begin{code}
  data Vec' (A : Type0) (n : Nat) : Type0 where
    nil   : n == 0 -> Vec A n
    cons  : A -> (m : Nat) -> Vec A m -> n == S m -> Vec A n
\end{code}

If we now try to define |head| by pattern matching on |Vec' A (S n)|,
we get the following:

\begin{code}
  head : (A : Type0) (n : Nat) -> Vec' A (S n) -> A
  head A n (nil p) = ?
  head A n (cons x m xs q) = x
\end{code}

The type of |p| is |S n == 0|, which is can be shown to be the empty
type, so the |nil| case can be ignored. Note that we also have |q : n
== S m|, corresponding to the dot pattern |.(S n)| in the previous
definition of |head|.

\todoi{Mention the Cockx paper and observations, work out how UIP becomes a problem}

\todoi{Cockx' work is very syntactic. My goal is to give a more
  semantic/internal account of pattern matching. Idea: create DSL for
  inductive types along with what pattern matching should look like.}

\section{Other inconsistencies with dependent pattern matching}

Another inconsistency in Agda's pattern matching mechanism has been
uncovered on the Agda mailing list\todo{citation}. Suppose we have a
type |Box| with one constructor:

\begin{itemize}
\item |wrap : (0 -> Box) -> Box|
\end{itemize}

Given that there is only one function |0 -> Box|, we notice that |Box|
itself must be a proposition. In fact, we have an equivalence between
|Box| and |0 -> Box|. If we have univalence or just propositional
extensionality, we have a proof |pf : Box == (0 -> Box)|. Consider the
following function:

\begin{code}
  noo : (X : Type0) -> (Box == X) -> X -> 0
  noo .Box refl (wrap f) = noo (0 -> Box) pf f
\end{code}

Note that we first should pattern match in the |Box == X| argument,
which forces |X| to be |Box|, allowing us to pattern match on the
third argument, which now has type |Box|. We can then recursively call
|noo| on the |f : 0 -> Box| we get from pattern matching, which makes
the definition look structurally recursive.

Using |noo| we can now define an inhabitant of |0| as follows:

\begin{code}
  bad : 0
  bad = noo (0 -> Box) pf (\ ())
\end{code}

\todoi{Some more on why this is evil and where the problem should
  be. Is there an error in Conor's work? Is there an assumption that
  we have missed?}

\printbibliography

\end{document}

