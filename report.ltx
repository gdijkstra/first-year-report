\documentclass[a4paper,10pt]{report}

%include report.fmt

\newcommand{\todoi}[1]{\todo[inline]{#1}}

\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10,protrusion=trues]{microtype}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[parfill]{parskip}
\usepackage{amsmath,amsthm,amssymb,stmaryrd}
\usepackage{todonotes}
\usepackage{tgpagella}
\usepackage{url}
\usepackage{xspace}

% Bibliography stuff
\usepackage[autostyle]{csquotes}

\usepackage[
    backend=biber,
    style=authoryear-icomp,
    natbib=true,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\addbibresource{latex-base/refs.bib}

\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing} % for snake lines
\usetikzlibrary{matrix}
% TikZ styles for drawing

\tikzstyle{snakeline} = [->, decorate, decoration={zigzag, pre length=0.1cm, post length=0.1cm, segment length=1.5mm, amplitude=.25mm}]
\tikzstyle{snakelin} = [->, decoration={zigzag, pre length=0.1cm, post length=0.1cm, segment length=1.5mm, amplitude=.25mm}]
\tikzset{node distance=2cm, auto}

\newcommand{\withoutk}{\xspace\verb+--without-K+\xspace}
\newcommand{\hott}{homotopy type theory\xspace}
\newcommand{\Hott}{Homotopy type theory\xspace}
\newcommand{\mltt}{Martin-L\"of's type theory\xspace}
\newcommand{\hit}{higher inductive type\xspace}
\newcommand{\Hit}{Higher inductive type\xspace}
\newcommand{\hits}{higher inductive types\xspace}
\newcommand{\Hits}{Higher inductive types\xspace}
\newcommand{\onehit}{$1$-HIT\xspace}
\newcommand{\onehits}{$1$-HITs\xspace}
\newcommand{\oneconstructor}{$1$-constructor\xspace}
\newcommand{\oneconstructors}{$1$-constructors\xspace}
\newcommand{\twohit}{$2$-HIT\xspace}
\newcommand{\twohits}{$2$-HITs\xspace}
\newcommand{\twoconstructor}{$2$-constructor\xspace}
\newcommand{\twoconstructors}{$2$-constructors\xspace}
\newcommand{\ie}{i.e.\xspace}

% Path composition from The Book.
\newcommand{\ct}{%
  \mathchoice{\mathbin{\raisebox{0.5ex}{$\displaystyle\centerdot$}}}%
             {\mathbin{\raisebox{0.5ex}{$\centerdot$}}}%
             {\mathbin{\raisebox{0.25ex}{$\scriptstyle\,\centerdot\,$}}}%
             {\mathbin{\raisebox{0.1ex}{$\scriptscriptstyle\,\centerdot\,$}}}
}

\title{Pattern matching in homotopy type theory}

\author{First year report \\\\ Gabe Dijkstra}

\date{\today}

\begin{document}

\maketitle

%if False
\begin{code}
module report where
\end{code}
%endif

\tableofcontents

\chapter*{Preface}
\label{ch:preface}
\addcontentsline{toc}{chapter}{Preface}

dingen foo

\chapter{Homotopy type theory}

Short introduction to homotopy type theory.

\begin{code}
  transport  :   (A : Type0)
                 (B : A -> Type0)
                 (x y : A)
                 (p : x eqA y)
             ->  B x -> B y
\end{code}

It can happen that we want to compare a value of type |B x| with
something of type |B y|, while having a proof of |x eqA y|. Using
|transport| we can transform the left hand side to something of type
|B y|, so we can compare the two. The notation for |transport A B x y
p u == v| is |u == v [B down p]|.

\chapter{Higher inductive types}
\label{ch:hit}

\Hott teaches us that we can regard types as topological spaces up to
homotopy. This leads us to the question of how we can represent
familiar spaces such as the circle and the torus in a type theoretic
way. So far several examples of what \hits
may look like have been explored, \todo{Citations} but formal scheme
has been devised as of yet. In this chapter we will explore ways to
formalise (restricted) notions of higher inductive types.

\todoi{More motivation}

\section{Examples}

\subsection{Circle}

The idea of \hits is that apart from providing ways of constructing
points, we also provide ways of constructing paths. For example, the
circle can be defined as the type generated by one point and one
non-trivial loop on that point: we define |Circle| to be the type with:

\begin{itemize}
\item a \emph{point} constructor |base : Circle|
\item a \emph{path} constructor |loop : base == base|
\end{itemize}

In order to eliminate out of this type, we need to provide a point in
our codomain and ensure that this point also has a corresponding loop:

\begin{code}
  Circle-elim : (Z : Type0) (zb : Z) (zloop : zb == zb) -> Circle -> Z
\end{code}

As with ordinary inductive types, the elimination principle should
also satisfy certain computation rules:

\begin{code}
  Circle-elim-comp-point  : (Z : Type0) (zb : Z) (zloop : zb == zb)
                          -> Circle-elim Z zb zloop base == zb
\end{code}

Apart from computation rules on points, we also need computation rules
on paths, witnessing the fact that every function defined in type
theory respects equality:

\begin{code}
  Circle-elim-comp-path  : (Z : Type0) (zb : Z) (zloop : zb == zb)
                         -> ap (Circle-elim Z zb zloop) == zloop
\end{code}

The above rules show us that when we want to define a function |f :
Circle -> Z|, we have to say what |f base| and |ap f loop| should be.

We also need the \emph{dependent} elimination principle on circles,
which is as one would expect:

\begin{code}
  Circle-elim-dep  : (Z : Circle -> Type0) (zb : Z base) (zloop : zb == zb [Z down loop]) 
                   -> (x : Circle) -> Z x
\end{code}

The computation rule for points is the same and the rule for paths is
almost the same: we only have to replace |ap| with |apd|.

\subsection{Propositional truncation}

Apart from having path constructors between point constructors, one
can also have \emph{variables} of the type being defined in left or
right hand sides of the equations. An example of this is propositional
truncation, or $(-1)$-truncation: the operation that creates a version
of a given type such that it is a \emph{proposition}: every two
elements are regarded as equal. Given |A : Type0|, the propositional
truncation of |A|, called |// A //| is defined by:

\begin{itemize}
\item a point constructor |[ _ ] : A -> // A //|
\item a path constructor |trunc : (x y : // A //) -> x == y|
\end{itemize}

with the elimination principle:

\begin{code}
  // A // - elim : (Z : Type0) (f : A -> Z) (trunc : (x y : Z) -> x == y) -> // A // -> Z
\end{code}

One might think that it should be enough that the image of |f| has to
be propositional, instead of the whole of |Z| and that the elimination
principle is too strict. However, any |f : A -> Z| can be factored as
|A -> Im f -> Z| where |Im f := (z : Z) times (a : A) times f a == z|
and the function |Im f -> Z| is just the first projection. If we can
show that |Im f| is propositional, then we get |// A // -> Im f| hence
|// A // -> Z|.

\subsection{Torus}

The two examples we have seen so far are special cases of \hits,
called \onehits, as the path constructors describe paths between
points of the type being defined, hence can be called
\emph{\oneconstructors} (with ordinary point constructors being
\emph{$0$-constructors}). We can also consider adding constructors for
paths between paths. One example of this is the following construction
of the torus, which has:

\begin{itemize}
\item a point constructor |base : Torus|
\item two non-trivial loops |p q : base == base|
\item such that their composition commutes: |t : p trans q == q trans p|
\end{itemize}

In order to eliminate from the torus, we not only need to provide a
base point |zb : Z| with two loops |zp zq : zb == zb|, but also a
proof that the loops commute, \ie a proof of |zp trans zq == zq trans
zp|.

\subsection{Set truncation}

In the above example one sees that the terms that can occur in the
constituents of the equations of higher path constructors are no
longer terms generated by the point constructors that can have
variables in them. They are terms built out of the path constructors
or variables from one level down, which can be combined with the
groupoid operations that come with equalities: transitivity, symmetry
and reflexivity.

One example in which the reflexivity term occurs in a $2$-constructor,
is $0$-truncation, or set truncation, which is given by:

\begin{itemize}
\item a point constructor |[ _ truncend0 : A -> // A ///|
\item a path constructor |trunc0 : (x : // A ///) (p : x == x) -> p == refl|
\end{itemize}

The path constructor forces axiom K to hold for the type |// A ///|,
making it a set. If we want to eliminate into |Z|, we need to make
sure that |Z| is a set:

\begin{code}
  // A ///-elim  :   (Z : Type0) 
                      (f : A -> Z)
                      (trunc : (x y : Z) -> (p : x == y) -> p == refl)
                  ->  // A /// -> Z
\end{code}

Just as with propositional truncation, it is actually enough for the
image of |f| to be a set and we can use the factorisation trick to get
the function |// A /// -> Z| we want.

\subsection{Hub-spokes construction}

It has been observed that one actually can do without the higher path
constructors \citep{Lumsdaine2012}: every $n$-HIT can be reduced to a
\onehit. For example, the disc may be realised as a circle with an
added point constructors (the hub) along with a continuous function of
paths from points on the circle to the hub (the spokes). More
precisely: the disc can be realised as the following \hit:

\begin{itemize}
\item a $0$-constructor |base : Disc|
\item a $1$-constructor |loop : base == base|
\item a $2$-constructor |fill : loop == refl|
\end{itemize}

The hub and spokes variant of this would be:

\begin{itemize}
\item a $0$-constructor |base : Disc|
\item a $0$-constructor |hub : Disc|
\item a $1$-constructor |loop : base == base|
\item a $1$-constructor |spokes : (x : Circle) -> f x == hub|
\end{itemize}

where |f : Circle -> Disc| is defined with |f base := base| and |ap f
loop := loop|. The filling in should be seen as gluing in the cone of
the circle, where the boundary of the circle is glued to the loop of
the disc.

The disc itself is not the most exciting example for this
construction, as it is a contractible type. However, replacing
$n$-constructors by a gluing operation of a cone of an $(n-1)$-sphere
means that we can replace any $n$-constructor by an
$(n-1)$-constructor, where $n > 1$. If we do this iteratively, we see
that we at most need $1$-constructors to describe our \hits.


\section{Containers}

Before we can start formalising a notion of \hits, we need a better
grip on what ordinary inductive types are. One way to look at
inductive types is as initial algebras of strictly positive
functors. For example, the list data type can be seen as the initial
algebra of the functor (having fixed some |A : Type0|) |ListFA X = 1 +
A times X|. An algebra for this functor is a type |Z : Type0| along
with a function |ListF Z -> Z|. An initial algebra |List : Type0| with
|sup : ListF List -> List| then satisfies the following universal
property for any algebra |theta : ListF Z -> Z| with |Z : Type0|:

\begin{center}
  \begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
 { |ListF List| & & |List| \\
   |ListF Z|    & & |Z|    \\ };

  \path[->] (m-1-1) edge[decorate] node[auto]       {|sup|} (m-1-3);
  \path[->] (m-1-1) edge[decorate] node[auto,swap]  {|F (elim theta)|} (m-2-1);
  \path[->] (m-1-3) edge[decorate] node[auto]       {|elim theta|} (m-2-3);
  \path[->] (m-2-1) edge[decorate] node[auto]       {|theta|} (m-2-3);
  \end{tikzpicture}
\end{center}

Unfolding the definition of |ListF| in the type of |sup| and applying
the universal property of sums shows us that |sup| is exactly the
familiar list constructors |nil| and |cons| combined. The existence of
|elim theta| for any |theta| gives us the non-dependent elimination
principle, which in this case, if we unfold the definition of |ListF|,
gives, we see that this is the familiar |fold| operation, albeit in
uncurried form. The commutativity of the diagram, \ie the fact that
|elim theta| is an algebra homomorphism, gives us the computation
rules for |elim theta|.

Initiality also means that two algebras satisfying the universal
property are isomorphic, so we can fix a candidate and talk about
\emph{the} initial algebra of a strictly positive functor.

The condition on functors that they need to be strictly positive is
necessary: if we allow for recursion to happen in negative positions,
we can write terms that do not normalise. We can encode the
non-normalising |(\ x . x x) (\ x . x x)| combinator from the untyped
$\lambda$-calculus using the initial algebra of the functor |F X = X
-> X|. Let us call the initial algebra of |F| |Term|, with its
constructor being |Lam : Term -> Term|.

\begin{code}
  omega : Term -> Term
  omega (Lam x) := x (Lam x)

  Omega : Term
  Omega = omega (Lam omega)
\end{code}

The condition that functors may not have recursive occurrences in
negative positions can be described as that the functor can be written
as using the following grammar:

\todoi{grammar from that one paper}

Formalising this grammar has several problems. Dealing with binders is
usually annoying as one has to use things such as de Bruijn
indices. One also has the ``problem'' that there are multiple
representations of the same functor: the constant functor can be
described as |`1| or |`1 `+ `0|, etc.

Another way to more succinctly formalise the notion of strictly
positive functors is as \emph{containers} \citep{Abbott2005}: a
container is a pair |S : Type0|, the type of \emph{shapes}, and |P : S
-> Type0|, the type of \emph{positions}, denoted |S lhd P|. The
container then describes the functor |sem(S lhd P) X = (s : S) times
(P s -> X)|, which is called the \emph{extension} of the container |S
lhd P|. \todoi{Show that it is a functor: functor laws hold
  definitionally, assuming eta laws for products?}

\begin{code}
  map : {C : Cont} {A B : Type0} (f : A -> B) -> sem(C) A -> sem(C) B
  map f (s , u) := (s , \ i -> f (u i))
\end{code}

The pros \todo{need better word for this} of this approach are that it
is easier to formalise in languages such as Agda and that if we have
two containers such that the extensions are equal, then the components
must be equal as well.

\subsection{Initial algebra of a container}

Containers give us a nice description of strictly positive functors,
but we also need their initial algebras. Suppose |sup : sem(C) WC ->
WC| is an initial algebra of the functor |sem(C)| for some container
|C|, then we have that the following diagram commutes for any other
algebra |theta : sem(C) Z -> Z|:

\begin{center}
  \begin{tikzpicture}
    \node (CWC)               {|sem(C) WC|};
    \node (CZ) [below of=CWC] {|sem(C) Z|};
    \node (WC) [right of=CWC] {|WC|};
    \node (Z)  [below of= WC] {|Z|};
    
    \draw[->] (CWC) to node {|sup|} (WC);
    \draw[->] (CWC) to node[swap] {|map C (elim theta)|} (CZ);
    \draw[->] (CZ) to node {|theta|} (Z);
    \draw[->] (WC) to node {|elim theta|} (Z);
  \end{tikzpicture}
\end{center}


If we unfold the definition of |sem(C)| we get the following:

\begin{itemize}
\item Term introduction: |sup : (s : S) times (P s -> WC) -> WC|

\item Elimination principle: |elim : {Z : Type0} (theta : s : S times
  (P s -> Z) -> WC -> Z|

\item Computation rule: for every |s : S|, |u : P s -> WC| and algebra
  |theta : (s : S) times (P s -> Z) -> Z| we get |elim theta (sup (s,
  u)) = theta (s, \ i -> elim theta (u i))|
\end{itemize}

which is the familiar \emph{W-type} construction from \todo{martin-lof
  citation}

Using this we can define the natural numbers as the initial algebra of
the following container:

\begin{code}
  S : Type0
  S := 1 + 1

  P : Type0
  P (inl tt) := 0
  P (inr tt) := 1
\end{code}

The |zero| and |succ| constructors can be defined as follows:

\begin{code}
  zero : WC
  zero := sup (inl tt) exfalso

  succ : WC -> WC
  succ n := sup (inr tt) (\ x -> n)
\end{code}

\todoi{Note something about the lack of unicity of the
  constructors. One needs to compare functions from an empty type into
  some other type. Adding this functionality to the type theory
  quickly makes the system undecidable. Check
  \verb+http://mazzo.li/epilogue/index.html%3Fp=324.html+.}

\subsection{Natural transformations}

One can think of what a good notion of morphism between containers
should be. Suppose |C0| and |C1| are containers and |X : Type0|, if we
want to define a function |sem(C0) X -> sem(C1) X|, we notice that,
unfolding the definitions, we have to provide two functions:

\begin{itemize}
\item |f : S0 -> S1|, a mapping between the shapes
\item |g : (s0 : S0) -> P1 (f s0) -> P0 s0|, such that we can map the
  positions in the other way around.
\end{itemize}

Let us define the type of container morphisms to be |Mor : Cont ->
Cont -> Type0| with |mk-mor : (f : S0 -> S1) (g : (s0 : S0) -> P1 (f
s0) -> P0 s0) -> Mor C0 C1|. The action of the container morphism on a
types can be defined as follows:

\begin{code}
  apply-mor : Mor C0 C1 -> (X : Type0) -> sem(C0) X -> sem(C1) X
  apply-mor (mk-mor f g) X (s , u) := (f s , \i -> u (g (f s) i))
\end{code}

It so happens that container morphisms are in fact natural
transformations. Given |C0 C1 : Container| and |X Y : Type0|, the
following commutes definitionally, where |eta := apply-mor m| for
some |m : Mor C0 C1|:

\begin{center}
  \begin{tikzpicture}
    \node (C0X)               {|sem(C0) X|};
    \node (C0Y) [below of=C0X] {|sem(C0) Y|};
    \node (C1X) [right of=C0X] {|sem(C1) X|};
    \node (C1Y) [below of=C1X] {|sem(C1) Y|};
    
    \draw[->] (C0X) to node[swap] {|map C0 f|} (C0Y);
    \draw[->] (C1X) to node {|map C1 f|} (C1Y);
    \draw[->] (C0X) to node {|eta X|} (C1X);
    \draw[->] (C0Y) to node {|eta Y|} (C1Y);
  \end{tikzpicture}
\end{center}

We have shown that every container morphism is a natural
transformation. It also the case that every natural transformation
between the extensions of two containers can be described by a
container morphism, as is shown in~\cite{Abbott2005}.

\subsection{Free monads}

If we want to describe the left and right hand sides of the equations
in a \oneconstructor, we need some way to talk about terms of a data
type with possibly free variables. One representation of this is given
by the free monad of the functor of which the data type at hand is the
initial algebra.

Given a container, we can construct the free monad of the extension of
the container as a new data type |Free|, given parameters |C : Cont|
and |X : Type0|, with point constructors\footnote{This data type can
  easily be described as W-type. Here we will use this notation along
  with pattern matching definitions to improve readability.}:

\begin{itemize}
\item |return : X -> Free C X|
\item |roll : sem(C) (Free C X) -> Free C X|
\end{itemize}

The |join| operation can be defined as follows:

\begin{code}
  join : {C : Cont} {X : Type0} -> Free C (Free C X) -> Free C X
  join {C} (return c)   := c
  join {C} (roll x)  := roll (map C join x)
\end{code}

The above definition does not immediately look like it is structurally
recursive, but if we inline the definition of |map|, it does becomes
the case.

Defining |return| and |join| is of course not enough: one needs to show
that these follow the monad laws. Before we can define the monad laws,
we need to show that |Free C| is a functor for every |C :
Cont|. Instead of defining a new |map| function for this data type and
showing that it satisfies the functor laws, we can instead create a
container out of this. We know that |sem(S lhd P) top = (s : S) times
P s -> top == S|. Using this fact, given a container |S lhd P|, we can
produce a new container, where |P*| is defined by structural
recursion:

\begin{code}
    S' : Type0
    S' := Free C top

    P' : S' -> Type0
    P' (ret tt)        := top
    P' (roll (s , u))  := (i : P s) times P' (u i)
\end{code}

We can now define an operation |_* : Cont -> Cont|. Using |sem(C *)|
instead of |Free C| means that we can reuse the definition of |map|
for containers and also the definition of natural transformations. We
need to make sure that this operation indeed gives us what we want:
for every |C : Cont| and |X : Type0|, |Free C X == sem(C *) X| must
hold.

\todoi{Talk about the functions that go in both directions.}


\subsubsection{Algebras for a free monad}

The free monads we are working with happen to be
\emph{algebraically-free}: there is an equivalence between the
category of algebras on |sem(C)| and the category of monad algebras on
|sem(C *)|. This fact is witnessed by the two operations:

\begin{code}
  lift : {C : Cont} {Z : Type0} -> (theta : sem(C) Z -> Z) -> sem(C *) Z -> Z
\end{code}

and

\begin{code}
  forget : {C : Cont} {Z : Type0} -> (theta : sem(C *) Z -> Z) -> sem(C) Z -> Z
\end{code}

\todoi{Operational understanding of these operations.}

Note that we are being a bit imprecise in the type of |forget|, the
algebra |theta| passed to |forget| needs to be a \emph{monad} algebra, \ie the
following must commute:

\begin{center}
  \begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
 { |sem(C *) (sem (C *) X)| & & |sem(C *) X| \\
   |sem(C *) X|    & & |X|    \\ };

  \path[->] (m-1-1) edge[decorate] node[auto]       {|map sem(C *) theta|} (m-1-3);
  \path[->] (m-1-1) edge[decorate] node[auto,swap]  {|join|} (m-2-1);
  \path[->] (m-1-3) edge[decorate] node[auto]       {|theta|} (m-2-3);
  \path[->] (m-2-1) edge[decorate] node[auto]       {|theta|} (m-2-3);
  \end{tikzpicture}
\end{center}

\begin{center}
  \begin{tikzpicture}
    \node (X) {|X|};
    \node (T)  [right of=X] {|sem (C *) X|};
    \node (X') [below of=T] {|X|};

    \draw[->] (X) to node {|return|} (T);
    \draw[->] (T) to node {|theta|} (X');
    \draw[->] (X) to node[swap] {|id|} (X');
  \end{tikzpicture}
\end{center}

One can easily show that |forget (lift theta) x == theta x| for any
|theta : sem (C) X -> X| and |x : sem( C *) X| by case distinction and
$\beta$-reduction. Showing that |lift (forget theta) x == theta x| for
|theta : sem (C *) X -> X| and |x : sem(C) X| is a bit more involved
and needs to use the fact that |theta| is a monad algebra. \todo{Maybe
  say what we actually have to do for equivalence of categories: this
  is only the object part of the functors}

As we have previously mentioned, the free monad of a functor can be
seen as the type of terms of the initial algebra of the same functor,
but with free variables of the given type. We have an instantiation
operation |inst| that takes a substitution |v : X -> W C| and a term
with free variables |x : sem(C *) X| and produces a |W C| by the
following composition:

\begin{center}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
    { |sem(C *) X| & & |sem(C *) (W C)| & & |W C| \\ } ;

    \draw[->] (m-1-1) edge[decorate] node[auto] {|map (C *) v|} (m-1-3);
    \draw[->] (m-1-3) edge[decorate] node[auto] {|lift sup|}    (m-1-5);
  \end{tikzpicture}
\end{center}

We also have a ``generalisation'' operation that goes in the other
direction:

\begin{code}
    gen : W C -> Free C X
    gen (sup x) := roll (map C gen x)
\end{code}

which satisfies the property that for every value |x : W C| and
instantation |v : X -> W C|, we have |inst v (gen x) == x|.

\section{Construction of \onehits}

We want to extend the concept of container to also be able to describe
the data needed for the \emph{path} constructors:

\begin{itemize}
\item |C0 : Cont| for the point constructors
\item |C1 : Cont| where the shapes |S1 : Type0| represent the shapes
  of the path constructors. The positions |P1 : S1 -> Type0| give the
  type of \emph{variables} for every shape.
\item |l r : (s : S1) -> sem(C0 *) (P1 s)| describe the left and right
  hand side of the path constructors, where |sem(C0 *) (P1 s)| should
  be seen as terms constructed using |W C0| with free variables coming
  from |P1 s|.
\end{itemize}

The point and path constructors are then as follows, if we call the
type we are defining |H|:

\begin{itemize}
\item |sup0 : sem(C0) H -> H|
\item |sup1 : (s : S1) -> (v : P1 s -> H) -> inst v (l s) == inst v (r s)|
\end{itemize}

\todoi{Note that the version of |inst| here is a bit different:
  instead of doing |lift sup|, we do |lift sup0|}

The argument of type |P1 s -> H| of the path constructor can be read
as the assignment of the free variables.

To eliminate out of a \onehit, we need to say what one needs to be
done with the point constructors and show that this is done in a way
that respects the $1$-constructors. This leads us to the following
elimination principle:

\begin{code}
  elim  :  (Z : Type0)
           (theta : sem(C0) Z -> Z)
           (rho :  (s : S1) -> (v : P1 s -> Z) -> 
                   lift theta (map (C0 *) v (l s)) == lift theta (map (C0 *) v (r s)))
        -> H -> Z
\end{code}

This principle should satisfy the following computation rule on point
constructors definitionally, given |m : sem(C0) H|:

\begin{code}
  elim Z theta rho (c0 m) = theta (map C0 (elim Z theta rho) m)
\end{code}

Just as the above computation rule explains how to reduce the
elimination principle applied to a canonical \emph{point}, we want a
computation rule that explains how to reduce the elimination principle
applied to a canonical \emph{path}. Just as the above refers to the
algebra |theta| on points, we expect the path computation rule to
refer to the ``algebra'' on paths. Both |sup1| as well as |rho| expect a
substitution, but |rho| expects a substitution into |Z| instead of
|H|, but any |v : X -> H| induces a |v' : X -> Z| by postcomposition
with |elim Z theta rho|. Using this, a somewhat sensible candidate for
the path computation rule is;

\begin{code}
  ap (elim Z theta rho) (sup1 s v) == rho s (\ x -> elim Z theta rho (v x))
\end{code}

However, the substitution However, this does not type check, as the
term on the left hand side has type |elim Z theta rho (inst v (l s))
== elim Z theta rho (inst v (r s))|, whereas the right hand side has
type |lift theta (C0 *) (\ x -> elim Z theta rho (v x)) (l s) == lift
theta (map (C0 *) (\ x -> elim Z theta rho (v x)) (r s))|.

This problem can be solved, but with this presentation it is difficult
to see what is actually happening, so the solution is postponed until
\todo{ref}.

\subsection{Examples}

\subsubsection{Circle}

As mentioned previously, the circle can be inductively defined as a
type with one point constructor (|base|) and one path constructor
(|loop : base == base|). 

\begin{itemize}
\item |C0 := 1 lhd (\ s -> 0)|, \ie one point constructor without any recursion.
\item |C1 := 1 lhd (\ s -> 0)|, \ie one point constructor without any variables or arguments.
\end{itemize}

We can now define |base := sup0 (tt , exfalso)|, which we need in the
data for the path constructor:

\begin{itemize}
\item |l s := gen base|
\item |r s := gen base|
\end{itemize}

We would like to be able to define |loop| as well with the following
type:

\begin{code}
  loop : base == base
  loop := sup1 tt exfalso
\end{code}

However, this does not type check.

\subsubsection{Propositional truncation}

Propositional truncation of a type |A : Type0| is given by one point
constructor |A -> // A //| and one path constructor |(x y : // A //)
-> x == y|. This means that |C0 := A lhd (\ s -> 0)| and |S1 := 1|
with |P1 x := Bool|. The truncation equation is given by |l x := sup
(inl true) exfalso| and |r x := sup (inl false)|. From this data we can
define |[ x ] := sup0 x exfalso| and |trunc x y := sup1 tt (\ b -> if b
then x else y) exfalso|.

\todoi{Show that the elimination principles just work out nicely.}

\subsection{Alternative construction}

An alternative, more conceptual presentation of \onehits, proposed by
Paolo Capriotti, is as follows:

\begin{itemize}
\item |C0 : Cont| for the point constructors
\item |C1 : Cont| for the path constructors
\item |left right : sem(C1) => sem(C0 *)| natural transformations
  describing the result types of the path constructors.
\end{itemize}

The constructors of the type |H| that we are defining, are:

\begin{itemize}
\item |sup0 : sem(C0) H -> H|
\item |sup1 : (x : sem(C1) H) -> lift sup0 (left !! x) == lift sup0 (right !! x)|
\end{itemize}

The description of the data looks similar to the previous approach,
except that the constituents of the equations are given in a seemingly
different form. However, providing a natural transformation |sem(C1)
=> sem(C0 *)| boils down to giving a function |(s : S1) -> sem(C0 *)
(P1 s)|: unfolding the definition of |sem(C0 *) (P1 s)| we get:

\begin{code}
  l : (s : S1) -> (s* : S0*) times (P0* -> P1 s)
\end{code}

which is the same as giving two functions:

\begin{code}
  l0 : S1 -> S0*
  l1 : (s : S1) -> P0* (l0 s) -> P1 s
\end{code}

We now need to show that |lift sup0 ((mk-nat l0 l1) !! (s , v)) == inst v
(l s)| (and similarly for |r|):

\begin{code}
  lift sup0 ((mk-nat l0 l1) !! (s , v)) ==
  lift sup0 (l0 s , \ i -> v (l1 (l0 s) i)) ==
  lift sup0 (map (C0 *) v (l0 s, l1 (l0 s)) ==
  lift sup0 (map (C0 *) v (l s)) ==
  inst v (l s)
\end{code}

The elimination principle is given by, which is also equivalent to the previous principle:

\begin{code}
  elim  :  (Z : Type0)
           (theta : sem(C0) Z -> Z)
           (rho :  (x : sem(C1) Z) -> lift theta (left !! x) == lift theta (right !! x))
        -> H -> Z
\end{code}

along with the computation rule for points:

\begin{code}
  elim Z theta rho (sup0 m) = theta (map C0 (elim Z theta rho) m)
\end{code}

Of course, we still have the problem of defining our path computation rule as:

\begin{code}
  ap (elim Z theta rho) (sup1 m) == rho (map C0 (elim Z theta rho) m)
\end{code}

However, with this representation it becomes a bit clearer what
diagrams have to commute in order for this to work. We have:

\begin{code}
  ap (elim Z theta rho) (sup1 m) : 
    elim Z theta rho (lift sup0 (left !! m)) == elim Z theta rho (lift sup0 (right !! m))
\end{code}

versus

\begin{code}
  rho (map C0 (elim Z theta rho) m) :
    lift theta (left !! map C0 (elim Z theta rho) m) == lift theta (right !! map C0 (elim Z theta rho) m) 
\end{code}

Asserting the equality to both left hand sides is the same as
asserting the commutativity of the following diagram:

\begin{center}
  \begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
 { |sem(C1) H| & & |sem(C0*) H| & & H \\
   |sem(C1) Z| & & |sem(C0*) Z| & & Z \\
 };

 \path[->] (m-1-1) edge[decorate] node[auto] {|left H|} (m-1-3);
 \path[->] (m-1-3) edge[decorate] node[auto] {|lift sup0|} (m-1-5);
 \path[->] (m-1-1) edge[decorate] node[auto, swap] {|map C0 (elim theta rho)|} (m-2-1);
 \path[->] (m-1-5) edge[decorate] node[auto] {|elim theta rho|} (m-2-5);
 \path[->] (m-2-1) edge[decorate] node[auto, swap] {|left Z|} (m-2-3);
 \path[->] (m-2-3) edge[decorate] node[auto, swap] {|lift theta|} (m-2-5);
  \end{tikzpicture}
\end{center}

We can fill in the diagram by adding an arrow |sem(C0 *) H -> sem(C0
*) Z|, namely |map (C0 *) (elim theta rho)|:

\begin{center}
  \begin{tikzpicture}[description/.style={fill=white,inner sep=2pt}]
    \matrix (m) [matrix of math nodes, 
                row sep=4.5em,
                column sep=3.5em,
                text height=1.5ex,
                text depth=0.25ex]
 { |sem(C1) H| & & |sem(C0*) H| & & H \\
   |sem(C1) Z| & & |sem(C0*) Z| & & Z \\
 };

 \path[->] (m-1-1) edge[decorate] node[auto] {|left H|} (m-1-3);
 \path[->] (m-1-3) edge[decorate] node[auto] {|lift sup0|} (m-1-5);
 \path[->] (m-1-1) edge[decorate] node[auto, swap] {|map C0 (elim theta rho)|} (m-2-1);
 \path[->] (m-1-3) edge[decorate] node[auto, swap] {|map (C0*) (elim theta rho)|} (m-2-3);
 \path[->] (m-1-5) edge[decorate] node[auto] {|elim theta rho|} (m-2-5);
 \path[->] (m-2-1) edge[decorate] node[auto, swap] {|left Z|} (m-2-3);
 \path[->] (m-2-3) edge[decorate] node[auto, swap] {|lift theta|} (m-2-5);
  \end{tikzpicture}
\end{center}

The resulting square on the left holds by naturality. The square on
the right is the statement that |elim theta rho| is an algebra
homomorphism from |lift sup0| to |lift theta|. This can be proven.

\todoi{Something on the technicalities: |Free C0 X| vs. |sem(C0 *)
  X|?}

\subsection{Closure properties}

We cannot define things like:

\begin{code}
  data A : Type0 where
    c : // A // -> A
\end{code}

\todoi{Something about why}

\todoi{Compare with strictly-positive functors: those just
  compose/nest/whatever.}

\todoi{Something about equations appearing in arguments of path
  constructors or whatever.}

\subsection{Constructing $2$-HITs}

\todoi{ In theory having only \onehits should be enough, since we have
  some sort of way to translate any higher definition to the lower
  one. But we still need some syntax for the higher ones, to be able
  to do the translation at all.}

\todoi{Goal: formalise the hub-and-spokes construction: take a code
  for a $2$-HIT and produce a code of $1$-HIT along with a proof that
  the interpretation of both codes yield equivalent types.}

\todoi{Note that one needs the notion of a free groupoid, which can be
  formalised using indexed \onehits as follows: explanation.}

\chapter{Dependent pattern matching}
\label{ch:patmatch}

In functional programming languages such as Haskell, one defines
functions on inductive types by using pattern matching. For example,
we can define the Ackermann function as follows:

\begin{code}
  Ack 0      n      = S n
  Ack (S m)  0      = Ack m (S 0)
  Ack (S m)  (S n)  = Ack m (Ack (S m) n)
\end{code}

When defining functions on inductive types in \mltt, we only have the
type its \emph{elimination principle} at our disposal. The
(non-dependent) elimination principle for the natural numbers is the
following:

\begin{code}
  Natelim : (Z : Type0) -> Z -> (Z -> Z) -> Nat -> Z
\end{code}

with computation rules

\begin{code}
  Natelim mz ms 0      = mz
  Natelim mz ms (S n)  = ms (Natelim mz ms n)
\end{code}

The definition of |Ack| using |Natelim| looks as follows:

\begin{code}
  Ack : Nat -> Nat -> Nat
  Ack = 
    Natelim  (Nat -> Nat) 
             (\ n -> S n)
             (\ ackm -> Natelim  Nat 
                                 (ackm (S Z)) 
                                 (\ ackSmn -> ackm ackSmn))
\end{code}

which is less readable then our original definition. It would be nice
if we added pattern matching to \mltt. However, we do need to be
careful when we do this. We have to be sure that our pattern matching
definitions are total. As it turns out, it is sufficient
\citep{Coquand1992} to check that the patterns are \emph{covering} and
that the recursive calls are done on \emph{structurally smaller}
arguments.

\todoi{Elaborate on this?}

\section{Inductive families}

In dependently-typed languages we can \emph{families} of inductive
types indexed by some other type, so-called \emph{inductive
  families}. We may want to define a family of list types, indexed by
the length:

\begin{code}
  data Vec (A : Type0) : Nat -> Type0 where
    nil   : Vec A 0
    cons  : A -> (n : Nat) -> Vec A n -> Vec A (S n)
\end{code}

|Vec| is a family of types \emph{parametrised} by |A : Type0| and
\emph{indexed} by |Nat|. Parameters are distinguished from indices in
that they occur uniformly in the result type of every constructor. In
the |Vec| example, the |A| occurs uniformly in both the result type of
|nil| as well as |cons|, whereas the index of type |Nat| is |0| in the
|nil| case and |S n| in the |cons| case.

Indices are a useful way of encoding invariants in our data types. We
can get a lot more information about a value we get just from its
type. If we for example get a value |l : Vec A (S n)| for some |A :
Type0| and |n : Nat|, we know that |l| cannot be |nil|, as that would
not type check. This kind of information can also help us when we
write functions on inductive families. Say we want to define the
function |head| on vectors, a definition that only makes sense on
non-empty vectors. We can specify this constraint in the type as
follows: |head : (A : Type0) (n : Nat) -> Vec A (S n) -> A|. As
mentioned before, we should only have to define what to do in the
|cons| case, as the |nil| case is impossible. The form of pattern
matching that takes into account the extra information one gets from
the indices as they occur in the type signature of the function, is
called \emph{dependent pattern matching}.

With dependent pattern matching, we can write |head| as follows:

\begin{code}
  head : (A : Type0) (n : Nat) -> Vec A (S n) -> A
  head A .(S n) (cons x n xs) = x
\end{code}

The dot in the pattern |.(S n)| indicates that there is no other
possible well-typed pattern other than |S n| for that argument. The
definition is also complete: the |nil| case is impossible as we have
previously mentioned.

\section{Uniqueness of identity proofs}

One important example of an indexed family is the identity type:

\begin{code}
data Id (A : Type0) (x : A) : A -> Type0 where
  refl : Id A x x
\end{code}

An inhabitant of |Id A x y| constitutes a proof that the terms |x| and
|y| are equal. Note that |refl : Id A x y| only type checks if |x| and
|y| are definitionally equal. To increase readability, we will
sometimes denote |Id A x y| by |x == y|, leaving the type argument
implicit.

Given this definition of equality, it might lead one to think that if
we have a proof |p : Id A x y|, it is necessarily unique. In fact,
using dependent pattern matching, we can easily prove this:

\begin{code}
  uip : (A : Type0) (x y : A) (p : x == y) -> p == refl
  uip A x .x refl = refl
\end{code}

Proving this using only the elimination principle of identity types
turns out to be impossible: one can construct models of \mltt with
identity types in which the uniquenes of identity proofs property does
not hold for every type. In \hott people are specifically interested
in the models that violate this principle, hence dependent pattern
matching in its unrestricted form is incompatible with \hott. We hope
to uncover what the root of this incompatibility is and whether we can
find a variation of dependent pattern matching that is not at odds
with \hott.

\section{Parametrising inductive families}

To get a better understanding of indexed familes and dependent pattern
matching, we will translate some definitions to a non-indexed,
parametrised family, using identity types. With identity types, one
can factor out the ``indexedness'' of a family and make it more
explicit what is going on when we try to eliminate out of such a
family.

For example, we can translate the vector type as follows:

\begin{code}
  data Vec' (A : Type0) (n : Nat) : Type0 where
    nil   : n == 0 -> Vec A n
    cons  : A -> (m : Nat) -> Vec A m -> n == S m -> Vec A n
\end{code}

If we now try to define |head| by pattern matching on |Vec' A (S n)|,
we get the following:

\begin{code}
  head : (A : Type0) (n : Nat) -> Vec' A (S n) -> A
  head A n (nil p) = ?
  head A n (cons x m xs q) = x
\end{code}

The type of |p| is |S n == 0|, which is can be shown to be the empty
type, so the |nil| case can be ignored. Note that we also have |q : n
== S m|, corresponding to the dot pattern |.(S n)| in the previous
definition of |head|.

\todoi{Mention the Cockx paper and observations, work out how UIP becomes a problem}

\todoi{Cockx' work is very syntactic. My goal is to give a more
  semantic/internal account of pattern matching. Idea: create DSL for
  inductive types along with what pattern matching should look like.}

\section{Other inconsistencies with dependent pattern matching}

Another inconsistency in Agda's pattern matching mechanism has been
uncovered on the Agda mailing list\todo{citation}. Suppose we have a
type |Box| with one constructor:

\begin{itemize}
\item |wrap : (0 -> Box) -> Box|
\end{itemize}

Given that there is only one function |0 -> Box|, we notice that |Box|
itself must be a proposition. In fact, we have an equivalence between
|Box| and |0 -> Box|. If we have univalence or just propositional
extensionality, we have a proof |pf : Box == (0 -> Box)|. Consider the
following function:

\begin{code}
  noo : (X : Type0) -> (Box == X) -> X -> 0
  noo .Box refl (wrap f) = noo (0 -> Box) pf f
\end{code}

Note that we first should pattern match in the |Box == X| argument,
which forces |X| to be |Box|, allowing us to pattern match on the
third argument, which now has type |Box|. We can then recursively call
|noo| on the |f : 0 -> Box| we get from pattern matching, which makes
the definition look structurally recursive.

Using |noo| we can now define an inhabitant of |0| as follows:

\begin{code}
  bad : 0
  bad = noo (0 -> Box) pf exfalso
\end{code}

\todoi{Some more on why this is evil and where the problem should
  be. Is there an error in Conor's work? Is there an assumption that
  we have missed?}

\printbibliography

\end{document}

